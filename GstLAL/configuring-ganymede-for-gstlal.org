# Created 2022-03-10 Thu 17:27
#+title: Summary: getting GstLAL to run on Ganymede
#+author: Marc van der Sluys
* Add Singularity dir to PATH
Add ~/cvmfs/oasis.opensciencegrid.org/mis/singularity/bin~ to ~PATH~ for singularity binary, e.g. in
=~/.bash_profile=:
#+begin_src bash
  PATH="$PATH:/cvmfs/oasis.opensciencegrid.org/mis/singularity/bin"
#+end_src
* Run in dcache
- run in ~/dcache/gravwav/~
- for high-throughput output from the jobs and long-term storage
- simply do ~mkdir /project/gravwav/$USER~, even if you cannot see the parent dir, and it will appear
  (automount).
- Users must be added to ~gravwav~ group?
* Define TMPDIR
- Must define ~$TMPDIR~, e.g. in =~/.bashrc=:
#+begin_src bash
  export TMPDIR="/tmp"
#+end_src
* Bind directories to Singularity
- ~/dcache~ and ~$TMPDIR~ are not available in your singularity container by default
  - you need to bind them using ~-B~
- when launching your job from its working directory, you can use ~$PWD~: ~-B $TMPDIR,$PWD~
* Install LIGO software packages
- ~proxy-x509-create~ == ~X509_USER_PROXY=x509_proxy ligo-proxy-init -p marc.vandersluys~:
  - NOTE: ligo username != Nikhef user name
- DONE: installed many lscsoft/ligo packages e.g. ligo-proxy-utils, lscsoft-all, -frame, -gds, -gstlal(-dev),
  -lalsuite-dev, -ldas-tools(-dev), -ligotools, ...
* DAG can be submitted, but first two individual jobs fail
- from the DAG log:
#+begin_src text
    01/21/22 17:18:09 From submit: ERROR: invalid value ("ON_SUCCESS") for WhenToTransferOutput. Please either 
    01/21/22 17:18:09 From submit: specify "ON_EXIT", or "ON_EXIT_OR_EVICT" and try again. 
    01/21/22 17:18:09 failed while reading from pipe.
  #+end_src
- Duncan McLeod: ~ON_SUCCESS~ introduced in HTCondor v8.9.7
  - WORKAROUND: run-dag-02 for fun: ~sed -i 's|ON_SUCCESS|ON_EXIT|g' *.sub~ and resubmit
  - later solution: upgrade HTCondor to 8.9.13
* Match HTCondor ClassAds with job requirements
- ISSUE: DAG is submitted, jobs are no longer killed, but do not run either (for days)
- ISSUE: Condor CPU requirements cannot be met, hence Condor is waiting for a CPU with the proper
  requirements to pop up, which neven happens
  - WORKAROUND: strip them: ~sed -i '/requirements/d' *.sub~
    - between ~make dag~ and ~make launch~
  - RESULT:
- ~condor_q -better-analyze~:
#+begin_src bash
    condor_q -better-analyze 896
    -- Schedd: visar.nikhef.nl : <145.107.7.239:9618?...
    The Requirements expression for job 896.000 is

        (has_avx2 && (CpuFamily is 6) && (HAS_SINGULARITY is true)) && (TARGET.Arch == "X86_64") && (TARGET.OpSys == "LINUX") && (TARGET.Disk >= RequestDisk) && (TARGET.Memory >= RequestMemory) && (TARGET.Cpus >= RequestCpus) &&
        (TARGET.HasFileTransfer)

    Job 896.000 defines the following attributes:

        DiskUsage = 5
        RequestCpus = 2
        RequestDisk = DiskUsage
        RequestMemory = 2000

    The Requirements expression for job 896.000 reduces to these conditions:

    	 Slots
    Step    Matched  Condition
    -----  --------  ---------
    [1]           0  CpuFamily is 6
    [3]           0  HAS_SINGULARITY is true
    [13]          0  TARGET.Cpus >= RequestCpus

    No successful match recorded.      
  #+end_src
- ISSUES:
  1. job requires CPU of family 6, which we don't have
     - this turns out to come from the ~ldas.yml~ config
     - SOLUTION: create ~nikhef.yml~ for CpuFamily 23
#+begin_src bash
         singularity exec $GSTLAL_IMG gstlal_grid_profile install  # Install GstLAL cluster profiles

         # In addition: add the Nikhef (Ganymede) GstLAL profile:
         wget https://raw.githubusercontent.com/MarcvdSluys/LIGO-Virgo-files/master/GstLAL/config/nikhef.yml
         singularity exec $GSTLAL_IMG gstlal_grid_profile install nikhef.yml
       #+end_src

  2. job requires Singularity, but ~HAS_SINGULARITY~ is false
     - added to HTCondor/signularity config:
#+begin_src conf
         HAS_SINGULARITY = HasSingularity
         STARTD_ATTRS = $(STARTD_ATTRS) HAS_SINGULARITY
       #+end_src
       - The second line solves the additional issue that the GstLAL ~*.sub~ files require
         ~HAS_SINGULARITY=?=True~ whereas our HTCondor advertises ~HasSingularity = true~

  3. job requires >1 CPU, Condor advertises as having 384 machines with 1 cpu each
     - SOLUTION: mend this in the cluster config (somehow)
* Ensure jobs run in a Singularity container
** Issue: jobs run, but crash immediately
- ~logs/split_injections_00000-847-0.err~:
#+begin_src text
    Traceback (most recent call last):
      File "/usr/bin/gstlal_injsplitter", line 83, in <module>
        cvs_entry_time=strftime('%Y/%m/%d %H:%M:%S'))
      File "/usr/lib64/python3.6/site-packages/ligo/lw/utils/process.py", line 109, in register_to_xmldoc
        process = proctable.RowType.initialized(program = program, process_id = proctable.get_next_id(), **kwargs)
      File "/usr/lib64/python3.6/site-packages/ligo/lw/lsctables.py", line 562, in initialized
        cvs_entry_time = lal.UTCToGPS(time.strptime(cvs_entry_time, "%Y-%m-%d %H:%M:%S +0000")) if cvs_entry_time is not None else None,
      File "/usr/lib64/python3.6/_strptime.py", line 559, in _strptime_time
        tt = _strptime(data_string, format)[0]
      File "/usr/lib64/python3.6/_strptime.py", line 362, in _strptime
        (data_string, format))
    ValueError: time data '2022/01/26 16:47:49' does not match format '%Y-%m-%d %H:%M:%S +0000'
  #+end_src
- same for ~$ /usr/bin/gstlal_injsplitter --nsplit 1 --usertag BNS --output-path filter/split_injections bns_injections.xml~
- not when prefixing ~singularity exec -B $TMPDIR,$PWD $GSTLAL_IMG~
- CONCLUSION: different version of GstLAL - command was not executed in singularity

** Diagnose the issue
- Run some simple copy commands and bash scripts to probe the environment on the compute nodes
- do (roughly) the same in the bash scripts as in the bare commands (2x)
- with and without singularity (2x)
- total: 4 incarnations
- NOTE: specify paths everywhere, e.g. ~/usr/bin/cp~
- RESULTS:
  1. no error when running outside a Singularity container
  2. errors when (trying to) run(ning) inside, e.g.
#+begin_src text
       Error from slot1_4@wn-lot-062.nikhef.nl: STARTER at 145.107.5.62 failed to send file(s) to
       <145.107.7.239:9618>: error reading from /var/lib/condor/execute/dir_7370/file2.out: (errno 2)
       No such file or directory; SHADOW failed to receive file(s) from <145.107.5.62:36330>
     #+end_src
  3. jobs are executed in the Condor scratch dir in ~/pbs/condor/execute/dir_X~, where X is some 4-5-digit
     number.
     - the input files are transferred there before execution, and the output files are transferred from
       there afterwards
  4. however, in a Singularity container, the job is run in the user's *HOME dir*
     - ISSUE: because the input files are not found there, the run cannot succeed (when I/O is involved)
     - SOLUTION: set ~SINGULARITY_TARGET_DIR = /srv~ in the HTCondor singularity config on the compute
       nodes
     - this adds ~--pwd /srv~ to the ~singularity exec ...~ call, causing the code to run in the Condor
       scratch dir, where the I/O files are
     - ~/srv~ is a *magical link* to the Condor scratch dir(!) and the ill-documented key to the solution
       of this issue
* Use correct data server
- First two jobs now run in a container for some seconds before crashing
- ~split_injections~ (job1) succeeds but ~gstlal_reference_psd~ (job2) fails
#+begin_src python
        Traceback (most recent call last):
        <SNAP>
      File "/usr/lib64/python3.6/http/client.py", line 974, in send
        self.connect()
      File "/usr/lib64/python3.6/http/client.py", line 946, in connect
        (self.host,self.port), self.timeout, self.source_address)
      File "/usr/lib64/python3.6/socket.py", line 704, in create_connection
        for res in getaddrinfo(host, port, 0, SOCK_STREAM):
      File "/usr/lib64/python3.6/socket.py", line 745, in getaddrinfo
        for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
    socket.gaierror: [Errno -2] Name or service not known
  #+end_src
  - ISSUE: using ~data-find-server: ldr.ldas.cit:80~ in ~config.yml~
    - ping ldr.ldas.cit doesn't work
  - SOLUTION: replace with ~data-find-server: datafind.ligo.org:443~
    - ~ping datafind.ligo.org~ works
* Ensure data directory is mounted and bound
- ~gstlal_reference_psd~ (job2) still fails:
#+begin_src bash
  ,**
  ERROR:framecpp_channeldemux.cc:819:gboolean sink_event(GstPad*, GstObject*, GstEvent*): assertion failed: (GST_ELEMENT(element)->numsrcpads > 0)
#+end_src
- Ron Tapia, Patrick Godwin:
  - no access to Frame files?
  - run test script
  - MvdS: ~gstlal_reference_psd_test.sh~ with command taken from ~full_inspiral_dag.sh~:
#+begin_src bash
      /usr/bin/gstlal_reference_psd -vvv --gps-start-time 1187000000 --gps-end-time 1187001000 --channel-name H1=GWOSC-16KHZ_R1_STRAIN --channel-name L1=GWOSC-16KHZ_R1_STRAIN --data-source frames --psd-fft-length 8 --frame-segments-name datasegments --frame-type H1=H1_GWOSC_O2_16KHZ_R1 --frame-type L1=L1_GWOSC_O2_16KHZ_R1 --data-find-server datafind.ligo.org:443 --frame-segments-file segments.xml.gz --write-psd H1L1-GSTLAL_REFERENCE_PSD-1187000000-1000.xml.gz

      ,**
      ERROR:framecpp_channeldemux.cc:819:gboolean sink_event(GstPad*, GstObject*, GstEvent*): assertion failed: (GST_ELEMENT(element)->numsrcpads > 0)
      /srv//condor_exec.exe: line 3:    15 Aborted                 /usr/bin/gstlal_reference_psd -vvv --gps-start-time 1187000000 --gps-end-time 1187001000 --channel-name H1=GWOSC-16KHZ_R1_STRAIN --channel-name L1=GWOSC-16KHZ_R1_STRAIN --data-source frames --psd-fft-length 8 --frame-segments-name datasegments --frame-type H1=H1_GWOSC_O2_16KHZ_R1 --frame-type L1=L1_GWOSC_O2_16KHZ_R1 --data-find-server datafind.ligo.org:443 --frame-segments-file segments.xml.gz --write-psd H1L1-GSTLAL_REFERENCE_PSD-1187000000-1000.xml.gz
    #+end_src
  - The file ~tmpj62n7hwo.cache~ is transferred back and contains
#+begin_src text
      H H1_GWOSC_O2_16KHZ_R1 1186996224 4096 file://localhost/cvmfs/gwosc.osgstorage.org/gwdata/O2/strain.16k/frame.v1/H1/1186988032/H-H1_GWOSC_O2_16KHZ_R1-1186996224-4096.gwf
      H H1_GWOSC_O2_16KHZ_R1 1187000320 4096 file://localhost/cvmfs/gwosc.osgstorage.org/gwdata/O2/strain.16k/frame.v1/H1/1186988032/H-H1_GWOSC_O2_16KHZ_R1-1187000320-4096.gwf
      L L1_GWOSC_O2_16KHZ_R1 1186996224 4096 file://localhost/cvmfs/gwosc.osgstorage.org/gwdata/O2/strain.16k/frame.v1/L1/1186988032/L-L1_GWOSC_O2_16KHZ_R1-1186996224-4096.gwf
      L L1_GWOSC_O2_16KHZ_R1 1187000320 4096 file://localhost/cvmfs/gwosc.osgstorage.org/gwdata/O2/strain.16k/frame.v1/L1/1186988032/L-L1_GWOSC_O2_16KHZ_R1-1187000320-4096.gwf
    #+end_src
- PG: try ~FrChannels /cvmfs/gwosc.osgstorage.org/gwdata/O2/strain.16k/frame.v1/H1/1186988032/H-H1_GWOSC_O2_16KHZ_R1-1186996224-4096.gwf~
#+begin_src bash
    $ FrChannels /cvmfs/gwosc.osgstorage.org/gwdata/O2/strain.16k/frame.v1/H1/1186988032/H-H1_GWOSC_O2_16KHZ_R1-1186996224-4096.gwf
    ,*** Cannot open file /cvmfs/gwosc.osgstorage.org/gwdata/O2/strain.16k/frame.v1/H1/1186988032/H-H1_GWOSC_O2_16KHZ_R1-1186996224-4096.gwf
    ,*** FrError: in FrFileIOpen Open file error (No such file or directory) for file /cvmfs/gwosc.osgstorage.org/gwdata/O2/strain.16k/frame.v1/H1/1186988032/H-H1_GWOSC_O2_16KHZ_R1-1186996224-4096.gwf

    $ lls /cvmfs/gwosc.osgstorage.org
    ls: cannot access /cvmfs/gwosc.osgstorage.org: No such file or directory
  #+end_src
- SOLUTION: ~/cvmfs/gwosc.osgstorage.org~ must be mounted
  - NOTE: it must also be added to the Singularity bind list
  - other urls?
* Upgrade HTCondor because of relative paths
- ~gstlal_median_of_psds~ (job3) fails.  ~gstlal_median_of_psds_00000-1099-0.err~:
#+begin_src python
  Traceback (most recent call last):
    File "/usr/bin/gstlal_median_of_psds", line 38, in <module>
      for ifo, psd in read_psd(f, verbose=options.verbose).items():
    File "/usr/lib64/python3.6/site-packages/gstlal/psd.py", line 239, in read_psd
      contenthandler=lal.series.PSDContentHandler
    File "/usr/lib64/python3.6/site-packages/ligo/lw/utils/__init__.py", line 427, in load_filename
      with open(filename, "rb") as fileobj:
  FileNotFoundError: [Errno 2] No such file or directory: 'reference_psd/11870/H1L1-GSTLAL_REFERENCE_PSD-1187000000-1000.xml.gz'
#+end_src
- ISSUE: ~preserve_relative_paths~: files in subdirs don't work properly
- SOLUTION: upgrade HTCondor to v8.9(.13) (from our v8.8)
  - this also adds the ~ON_EXIT~ flag for ~WhenToTransferOutput~
