# Created 2021-11-26 Fri 08:31
#+title: New GstLAL workflow
#+author: Marc van der Sluys
- [2021-11-23 Tue]
- https://lscsoft.docs.ligo.org/gstlal/cbc_analysis.html

* Set up and enter singularity container
- https://lscsoft.docs.ligo.org/gstlal/installation.html#singularity-container
  #+begin_src bash
    # Build writable container in subdir gstlal-dev-container/:
    singularity build --sandbox --fix-perms gstlal-dev-container docker://containers.ligo.org/lscsoft/gstlal:master
    mkdir gstlal-dev-container/hdfs gstlal-dev-container/archive gstlal-dev-container/cvmfs  # They may be needed later

    # Enter container (needed?):
    singularity run --writable gstlal-dev-container
  #+end_src

* Download and build the GstLAL dependencies
#+begin_src bash
  mkdir gstlal-deps && cd gstlal-deps
  wget https://git.ligo.org/lscsoft/gstlal/-/raw/master/gstlal-inspiral/share/post_O3/optimized/Makefile.ligosoftware_gcc_deps -O Makefile  # Download Makefile
  less Makefile  # Verify the make instructions, `q` to exit

  make deps_env.sh  # make deps_env.sh
  ls -l  # check the result
  source deps_env.sh  # source ('run') the script
  time make 1> make.out 2> make.err &  # build the dependencies, ~16 min
  tail -f make.out  # Follow installation (Ctrl-C to kill)
  tail -f make.err  # Follow progress bit more relaxedly
#+end_src

* Downloading and building GstLAL
Get GstLAL:
#+begin_src bash
  cd ..  # Get out of the gstlal-deps dir
  git clone git@git.ligo.org:lscsoft/gstlal.git    # git/ssh - need key setup?  ~30s  
  # git clone https://git.ligo.org/lscsoft/gstlal.git  # can't push over https?
#+end_src
- the ~git/ssh~ version should use your ssh key if properly set up
- it will ask your passphrase several times, so a key agent is already useful here

Build GstLAL:
#+begin_src bash
  mkdir gstlal-build && cd gstlal-build/
  wget https://git.ligo.org/lscsoft/gstlal/-/raw/master/gstlal-inspiral/share/post_O3/optimized/Makefile.ligosoftware_gcc_gstlal -O Makefile  # Download Makefile
  less Makefile  # Verify the make instructions
#+end_src
Edit ~Makefile~:
1. set the ~DEPS_DIR:=~ variable to the directory in which you built the dependencies,
   e.g. ~../gstlal-deps~
2. edit the ~GSTLAL_GIT_BRANCH~ variable if you want a different branch than ~master~

Build GstLAL:
#+begin_src bash
  make env.sh
  source env.sh
  time make -f Makefile 1> make.out 2> make.err &  # ~25 min
#+end_src
- this downloads and builds:
  1. lalsuite: lal, lalframe, lalmetaio, lalsimulation, lalburst, lalinspiral, lalpulsar, lalinference, lalapps
  2. gstlal: gstlal, gstlal-ugly, gstlal-burst, gstlal-calibration, gstlal-inspiral

* Set up the workflow (DAG)
https://lscsoft.docs.ligo.org/gstlal/cbc_analysis.html

** Create a dir and download files
Download a default config file, mass model and template bank:
#+begin_src bash
  cd ..  # Get out of the gstlal-build dir
  mkdir run-DAG-01 && cd run-DAG-01

  curl -O https://git.ligo.org/gstlal/offline-configuration/-/raw/main/bns-small/config.yml
  curl -O https://git.ligo.org/gstlal/offline-configuration/-/raw/main/bns-small/mass_model/mass_model_small.h5
  curl -O https://git.ligo.org/gstlal/offline-configuration/-/raw/main/bns-small/bank/gstlal_bank_small.xml.gz
#+end_src

** Install the site-specific profiles
- Needed only once per user/cluster

#+begin_src bash
  gstlal_grid_profile install
  # singularity exec <image> gstlal_grid_profile install  # Same effect
  gstlal_grid_profile list  # List installed profiles
#+end_src
This will install ~*.yml~ files in =~/.config/gstlal/=.

** Edit config.yml
Set e.g.:
#+begin_src yaml
  start: 1187000000
  stop: 1187100000

  instruments: H1L1

  data:
    template-bank: gstlal_bank_small.xml.gz

  prior:
    mass-model: mass_model_small.h5

  summary:
    webdir: ~/public_html/run-01
#+end_src

If your username doesn't match your LIGO albert.einstein name, you need to provide the latter:
#+begin_src yaml
  condor:
    accounting-group-user: albert.einstein
#+end_src

You may have to add or update the path to the singularity image:
#+begin_src yaml
  condor:
    singularity-image: /cvmfs/singularity.opensciencegrid.org/lscsoft/gstlal:master            
#+end_src

If running on an LDAS cluster rather than OSG, replace the profile:
#+begin_src yaml
  condor:
    profile: ldas
#+end_src

See https://lscsoft.docs.ligo.org/gstlal/cbc_analysis.html#analysis-configuration for more details on the
 configuration file.

** Create the Makefile
#+begin_src bash
  gstlal_inspiral_workflow init -c config.yml
  # singularity exec <image> gstlal_inspiral_workflow init -c config.yml  # Doesn't work
  # singularity exec <image> gstlal_inspiral_workflow init -c config.yml -w injection  # Injection only
#+end_src
This creates a file called ~Makefile~

** Set up a proxy if accessing non-public (GWOSC) data
#+begin_src bash
  X509_USER_PROXY=/path/to/x509_proxy ligo-proxy-init -p albert.einstein
#+end_src
- This asks for your LIGO password and creates a file called ~x509_proxy~ at the indicated location with
  certificates and a private key.
- Note that this must be run outside Singularity.
- Check: once per user/cluster?

Edit ~config.yml~ and set the correct path to the proxy file:
#+begin_src yaml
  source:
    x509-proxy: /path/to/x509_proxy
#+end_src

** Build the workflow/DAG file for submission
We need to select the whitening type using the environment variable ~GSTLAL_FIR_WHITEN~.  The value 0 sets the
traditional acausal whitening filter, 1 enables causal whitening.

#+begin_src bash
  export GSTLAL_FIR_WHITEN=0  # Set to 0 or 1
  make dag
  # singularity exec -B $TMPDIR <image> make dag
#+end_src
- This creates a list of files and subdirectories, amongst which Condor submission scripts (~*.sub~) and
  DAGMan files (~*.dag~).

*** Possible issues
1. When running ~make dag~  (w/o singularity):
   - ImportError: No module named _lal
     - goes away after trying a few times

* Launch the workflows/DAG
#+begin_src bash
  make launch  # Submit your DAG
  condor_q     # Monitor your DAG
#+end_src
- ~make launch~ runs ~condor_submit_dag~ and should report something like ~1 job(s) submitted to cluster xxx~
- ~condor_q~ should show a few dozen to several hundred jobs, probably idle and perhaps running.
- there should be a file called ~*.dag.dagman.out~ with status output.  You can follow what's going on with
  e.g. ~tail -f full_inspiral_dag.dag.dagman.out~
- typical run time is in the order of hours, depending on your settings and cluster load.

** Possible issues
1. When running ~make launch~
   #+begin_src bash
     ERROR: store_cred of LOCAL credential failed - The credmon did not process credentials within the timeout period
     ERROR: condor_submit failed; aborting.
   #+end_src
   - did you set up your proxy correctly?

* Generate the summary page
#+begin_src bash
  make summary
  # singularity exec -B $TMPDIR <image> make summary
#+end_src
- The results from ldas Caltech will show up in https://ldas-jobs.ligo.caltech.edu/~albert.einstein/

* Resuming work in a new shell
If you log in in a new shell, the environment variables you had set will be gone.  Hence, you will have
resource one of the env files.
1. before (re)building GstLAL:
   #+begin_src bash
     cd gstlal-deps
     source deps_env.sh
     cd -
   #+end_src
2. before (re)creating a (new) DAG:
   1. Resource the GstLAL environment:
      #+begin_src bash
        cd gstlal-build
        source env.sh
        cd -
      #+end_src
   2. Resetup your proxy
      #+begin_src bash
        X509_USER_PROXY=/path/to/x509_proxy ligo-proxy-init -p albert.einstein
      #+end_src
