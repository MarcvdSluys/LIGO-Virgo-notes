# Created 2021-12-03 Fri 12:14
#+title: New GstLAL workflow
#+author: Marc van der Sluys
- [2021-11-23 Tue]
- https://lscsoft.docs.ligo.org/gstlal/cbc_analysis.html

* Choose a singularity image/container for GstLAL
- A Singularity image or container contains a GNU/Linux system with (hopefully) the environment we need.  The
  cluster environment is replaced with that of the container, except for e.g. your home directory.
- Most important commands:
  #+begin_src bash
    singularity run <image>  # Replace the cluster environment with that of the container
    singularity exec <image> <command>  # Run <command> in the <image> environment, then return
  #+end_src
- See https://sylabs.io/guides/latest/user-guide/ for more details.
- You will have to choose between a *reference* (default, already setup) container and a *development*
  container.

** Use a reference image/container
#+begin_src bash
  MYIMAGE="/cvmfs/singularity.opensciencegrid.org/lscsoft/gstlal:master"     # Default GstLAL master
  MYIMAGE="/home/patrick.godwin/gstlal/offline/osg_small/gstlal-dev-210902"  # Patrick Godwin's version
#+end_src

** Set up a singularity image/container for GstLAL development
- https://lscsoft.docs.ligo.org/gstlal/installation.html#singularity-container
  #+begin_src bash
    # Pull a writable development container with GstLAL  installed in subdir gstlal-dev-container/:
    singularity build --sandbox --fix-perms gstlal-dev-container docker://containers.ligo.org/lscsoft/gstlal:master
    mkdir gstlal-dev-container/hdfs gstlal-dev-container/archive gstlal-dev-container/cvmfs  # They may be needed later

    MYIMAGE="$PWD/gstlal-dev-container"

    # Enter container (not needed?, test wether it works, exit with exit):
    # singularity run --writable $MYIMAGE  # gstlal-dev-container
  #+end_src

* Set up the workflow/DAG
https://lscsoft.docs.ligo.org/gstlal/cbc_analysis.html

** Create a dir and download files
Download a default config file, mass model and template bank:
#+begin_src bash
  # cd ..  # Get out of the gstlal-build dir
  mkdir run-DAG-01 && cd run-DAG-01

  curl -O https://git.ligo.org/gstlal/offline-configuration/-/raw/main/bns-small/config.yml
  curl -O https://git.ligo.org/gstlal/offline-configuration/-/raw/main/bns-small/mass_model/mass_model_small.h5
  curl -O https://git.ligo.org/gstlal/offline-configuration/-/raw/main/bns-small/bank/gstlal_bank_small.xml.gz
#+end_src

** Install the site-specific profiles
- Needed only *once* per user/cluster:
  #+begin_src bash
    singularity exec $MYIMAGE gstlal_grid_profile install  # Install profiles
    singularity exec $MYIMAGE gstlal_grid_profile list     # List installed profiles
  #+end_src
- This installs (and lists) ~*.yml~ files in =~/.config/gstlal/=.

** Edit config.yml
Set e.g.:
#+begin_src yaml
  start: 1187000000
  stop: 1187100000

  instruments: H1L1

  data:
    template-bank: gstlal_bank_small.xml.gz

  prior:
    mass-model: mass_model_small.h5

  summary:
    webdir: ~/public_html/run-01
#+end_src

If your username doesn't match your LIGO albert.einstein name, you need to provide the latter:
#+begin_src yaml
  condor:
    accounting-group-user: albert.einstein
#+end_src

You may have to add or update the path to the singularity image.  Make sure that this is the same as stored in
~$MYIMAGE~.
#+begin_src yaml
  condor:
    singularity-image: /cvmfs/singularity.opensciencegrid.org/lscsoft/gstlal:master            
#+end_src

If running on an LDAS cluster rather than OSG, replace the profile:
#+begin_src yaml
  condor:
    profile: ldas
#+end_src

Do you need to change the data server?  E.g.:
#+begin_src yaml
  source:
    data-find-server: ldr.ldas.cit:80
#+end_src

Reduced-order-model (ROM) waveforms present a faster version of a waveform model by storing a large number of
precomputed waveforms and performing smart interpolation between them.  The ROM files store these waveforms.
You can think of these as the weights of a rudimental machine-learning model.
#+begin_src yaml
  directives:
    environment: '"LAL_DATA_PATH=/home/cbc/ROM_data SINGULARITY_BIND=/home/cbc/ROM_data"'
#+end_src

See https://lscsoft.docs.ligo.org/gstlal/cbc_analysis.html#analysis-configuration for more details on the
 configuration file.

** Create the workflow/DAG Makefile
#+begin_src bash
  singularity exec $MYIMAGE gstlal_inspiral_workflow init -c config.yml
  # singularity exec $MYIMAGE gstlal_inspiral_workflow init -c config.yml -w injection  # Injection only
#+end_src
This creates a file called ~Makefile~

** Set up a proxy if accessing non-public (GWOSC) data
#+begin_src bash
  X509_USER_PROXY=/path/to/x509_proxy ligo-proxy-init -p albert.einstein
#+end_src
- This asks for your LIGO password and creates a file called ~x509_proxy~ at the indicated location with
  certificates and a private key.
- Note that this must be run *outside* the Singularity container.
- The proxy is valid for a few hours *(CHECK: correct?)*
- Note that you can use the ~proxy-x509-create~ alias to create ~x509_proxy~ in the current directory. [fn:1]

Edit ~config.yml~ and set the correct path to the proxy file:
#+begin_src yaml
  source:
    x509-proxy: /path/to/x509_proxy
#+end_src

[fn:1] https://github.com/MarcvdSluys/MyTerminalConfig/blob/master/bashrc_ligo

** Build the workflow/DAG file for submission
We need to select the whitening type using the environment variable ~GSTLAL_FIR_WHITEN~.  The value 0 sets the
traditional acausal whitening filter, 1 enables causal whitening.

#+begin_src bash
  export GSTLAL_FIR_WHITEN=0  # Set to 0 or 1
  singularity exec -B $TMPDIR $MYIMAGE make dag
#+end_src
- This creates a list of files and subdirectories, amongst which Condor submission scripts (~*.sub~) and
  DAGMan files (~*.dag~).
- Note: ~$TMPDIR~ is set when you login.

*** Possible issues
1. When running ~make dag~  (w/o singularity only?):
   - ImportError: No module named _lal
     - goes away after trying a few times

* Launch the workflow/DAG
#+begin_src bash
  make launch  # Submit your DAG
  condor_q     # Monitor your DAG
#+end_src
- Note: run *outside* the Singularity image
- ~make launch~ runs ~condor_submit_dag~ and should report something like ~1 job(s) submitted to cluster xxx~
- ~condor_q~ should show a few dozen to several hundred jobs, probably idle and perhaps running.
- there should be a file called ~*.dag.dagman.out~ with status output.  You can follow what's going on with
  e.g. ~tail -f full_inspiral_dag.dag.dagman.out~
- typical run time is in the order of hours, depending on your settings and cluster load.

** Possible issues
1. When running ~make launch~
   #+begin_src bash
     ERROR: store_cred of LOCAL credential failed - The credmon did not process credentials within the timeout period
     ERROR: condor_submit failed; aborting.
   #+end_src
   - did you set up your proxy correctly?

* Generate the summary page
#+begin_src bash
  make summary
  # singularity exec -B $TMPDIR <image> make summary
#+end_src
- The results from ldas Caltech will show up in https://ldas-jobs.ligo.caltech.edu/~albert.einstein/

* Resuming work in a new shell
If you log in in a new shell, the environment variables you had set will be gone.  Hence, you will have
resource one of the env files.
1. before (re)building GstLAL:
   #+begin_src bash
     cd gstlal-deps
     source deps_env.sh
     cd -
   #+end_src
2. before (re)creating a (new) DAG:
   1. Resource the GstLAL environment:
      #+begin_src bash
        cd gstlal-build
        source env.sh
        cd -
      #+end_src
   2. Resetup your proxy
      #+begin_src bash
        X509_USER_PROXY=/path/to/x509_proxy ligo-proxy-init -p albert.einstein
      #+end_src

* Submitting a rescue dag
- the file ~file.dag.dagman.out~ or similar will show whether any of the nodes failed (near the end)
- if this is the case, a ~file.dag.rescueXXX~ file will be created (where ~XXX~ is ~001~, ~002~, etc.)
- the DAG can be resubmitted using the original DAG file: ~condor_submit_dag file.dag~
- Q: is this the correct way?  Is this equivalent to redoing ~make launch~?

* Diagnosing and handling issues
- The file ~<name>_dag.dag.dagman.out~ contains output of your job.

** Final nodes status
You can use e.g. dagman-out-final-status [fn:2] to grep the final status of the nodes:
#+begin_src bash
  $ dagman-out-final-status

  full_inspiral_dag.dag.dagman.out:
  12/01/21 14:09:45 Of 150 nodes total:
  12/01/21 14:09:45  Done     Pre   Queued    Post   Ready   Un-Ready   Failed
  12/01/21 14:09:45   ===     ===      ===     ===     ===        ===      ===
  12/01/21 14:09:45    89       0        0       0       0         41       20
  12/01/21 14:09:45 0 job proc(s) currently held
#+end_src
In this case, 20 nodes failed

[fn:2] https://github.com/MarcvdSluys/MyTerminalConfig/blob/master/bin/dagman-out-final-status

** List of errors
A list of errors, if any, can be shown using e.g. dagman-out-list-errors [fn:3]:
#+begin_src bash
  $ dagman-out-list-errors

  full_inspiral_dag.dag.dagman.out:
  12/01/21 14:09:45 ---------------------- Job ----------------------
  12/01/21 14:09:45       Node Name: cluster_triggers_by_snr.00000
  12/01/21 14:09:45            Noop: false
  12/01/21 14:09:45          NodeID: 24
  12/01/21 14:09:45     Node Status: STATUS_ERROR
  12/01/21 14:09:45 Node return val: 1
  12/01/21 14:09:45           Error: Job proc (245431516.0.0) failed with status 1 (after 3 node retries)
  12/01/21 14:09:45 Job Submit File: cluster_triggers_by_snr.sub
  12/01/21 14:09:45           Retry: 3
  12/01/21 14:09:45  HTCondor Job ID: (245431516.0.0)
  12/01/21 14:09:45 PARENTS: gstlal_inspiral.00000 WAITING: 0 CHILDREN: gstlal_inspiral_calc_likelihood.00000
  etc...
#+end_src

Note that in this example, the Node name lists ~cluster_triggers_by_snr.XXXXXX~ as the culprit.  In the
subdirectory ~logs/~ you can find files called ~cluster_triggers_by_snr_XXXXX-YYYYYYYYY-Z.err~ containing more
details (e.g. tracebacks):
#+begin_src bash
  less logs/cluster_triggers_by_snr_00000-245431516-0.err

  Traceback (most recent call last):
  ...
#+end_src

[fn:3] https://github.com/MarcvdSluys/MyTerminalConfig/blob/master/bin/dagman-out-list-errors
